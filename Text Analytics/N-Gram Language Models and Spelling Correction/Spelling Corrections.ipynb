{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc7de8f",
   "metadata": {},
   "source": [
    "# Spelling Correction\n",
    "\n",
    "> *Text Analytics*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "i) Implement a bigram and a trigram language model for sentences, using Laplace smoothing. In practice, n-gram language models compute the sum of the logarithms of the n-gram probabilities of each sequence, instead of their product and you should do the same. Assume that each sentence starts with the pseudo-token *start* (or two pseudo-tokens *start1*, *start2* for the trigram model) and ends with the pseudo-token *end*. Train your models on a training subset of a corpus (e.g., a subset of a corpus included in NLTK – see http://www.nltk.org/). Include in the vocabulary only words that occur, e.g., at least 10 times in the training subset. Use the same vocabulary in the bigram and trigram models. Replace all out-of-vocabulary (OOV) words (in the training, development, test subsets) by a special token *UNK*. Alternatively, you may want to use BPEs instead of words (obtaining the BPE vocabulary from your training subset) to avoid unknown words. \n",
    "\n",
    "See Section 2.4.3 (“Byte-Pair Encoding for Tokenization”) of the 3rd edition of Jurafsky & Martin’s book (https://web.stanford.edu/~jurafsky/slp3/).\n",
    "\n",
    "For more information, check https://huggingface.co/transformers/master/tokenizer_summary.html.\n",
    "\n",
    "ii) Estimate the language cross-entropy and perplexity of your two models on a test subset of the corpus, treating the entire test subset as a single sequence of sentences, with *start* (or *start1*, *start2*) at the beginning of each sentence, and *end* at the end of each sentence. Do not include probabilities of the form P(*start*|…) or P(*start1*|…), P(*start2*|…) in the computation of cross-entropy and perplexity, since we are not predicting the start pseudo-tokens; but include probabilities of the form P(*end*|…), since we do want to be able to predict if a word will be the last one of a sentence. You must also count *end* tokens (but not *start*, *start1*, *start2* tokens) in the total length N of the test corpus.\n",
    "\n",
    "iii) Develop a context-aware spelling corrector using your bigram language model and a beam search decoder. If you are keen, you can also try using your trigram model. You can use the inverse of the Levenshtein distance between 𝑤𝑖,𝑡𝑖 as 𝑃(𝑤𝑖|𝑡𝑖).\n",
    "\n",
    "\n",
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a8d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import treebank\n",
    "from nltk import ngrams\n",
    "from nltk import edit_distance\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud\n",
    "from random import sample\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#nltk.download('treebank')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b793d87",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For the purpose of this exercise we are going to use **The Penn Treebank** dataset, which is a diverse and extensive collection of text genres, encompassing news articles, fiction, and academic writings, totaling over 4.5 million words. Annotated with part-of-speech tags, syntactic structures, and named entities, it provides a rich resource for natural language processing tasks. Notably, the dataset includes detailed sentence structures, offering phrase structure trees that are invaluable for tasks involving syntax and grammar. Widely utilized in research, the Penn Treebank has significantly influenced the development of natural language processing algorithms and serves as a benchmark dataset for evaluating models across various linguistic domains.\n",
    "\n",
    "##### *Function to process the dataset and create the vocabulary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b829dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_corpus_vocabulary(dataset,stopwords):\n",
    "    \n",
    "    # Get words from the dataset\n",
    "    dataset_words = dataset.words()\n",
    "\n",
    "    # Get words frequency\n",
    "    fdist = nltk.FreqDist(dataset_words)\n",
    "\n",
    "    # Keep words with frequency >=10\n",
    "    vocabulary = list(filter(lambda x:x[1]>=10,fdist.items()))\n",
    "\n",
    "    # map all out-of-vocabulary wordws to *UNK*\n",
    "    mapping = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    for word in vocabulary:\n",
    "        mapping[word[0]] = word[0]\n",
    "    \n",
    "    #create corpus\n",
    "    corpus = []\n",
    "    for sentence in dataset.sents():\n",
    "        corpus.append([mapping[word] for word in sentence])\n",
    "    \n",
    "    # remove stopwords from vocabulary\n",
    "    vocabulary = [w for w in vocabulary if not w in stopwords]\n",
    "    \n",
    "    return corpus,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac6db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stopword list and extend it to include puncuation marks and more common words\n",
    "stopwords_list = [stopwords.words('english')]\n",
    "stopwords_list.extend(string.punctuation)\n",
    "stopwords_list.extend([\"the\",\"of\",\"and\",\"to\",\"The\",\"for\",\"by\",\"*-1\",\"in\",\"0\",\"is\",\"*U*\",\"*T*-1\",\"a\",\"'s\",\"as\",\"*-2\",\"on\",\"it\",\"*T*-2\",\"at\",\"with\",\"from\",\"``\",\"''\",\"that\",\"Mr.\",\"are\",\"its\",\"n't\",\"have\",\"was\",\"be\",\"an\",\"has\",\"--\",\"will\",\"said\",\"or\",\"he\",\"this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69e797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute\n",
    "corpus, vocabulary = get_dataset_corpus_vocabulary(treebank,stopwords_list)\n",
    "\n",
    "#split to training - development and test set\n",
    "training = corpus[0:round(0.6*len(corpus))]\n",
    "development = corpus[round(0.6*len(corpus)):round(0.8*len(corpus))]\n",
    "testing = corpus[round(0.8*len(corpus)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3644c4",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "##### *Functions for n-gram counters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de36186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_counter(sentences):\n",
    "    unigram_counter = Counter()\n",
    "\n",
    "    for sent in sentences:\n",
    "        # Update the unigram counter\n",
    "        unigram_counter.update([(gram,) for gram in [\"<s>\"] + sent])\n",
    "    \n",
    "    return unigram_counter\n",
    "\n",
    "def calculate_bigram_counter(sentences):\n",
    "    bigram_counter = Counter()\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Update the bigram counter\n",
    "        bigram_pad_sent = [\"<s>\"] + sent +  ['<e>']    \n",
    "        bigram_counter.update([(gram1, gram2) for gram1, gram2 in zip(bigram_pad_sent, bigram_pad_sent[1:])])\n",
    "   \n",
    "    return bigram_counter \n",
    "\n",
    "def calculate_trigram_counter(sentences):\n",
    "    trigram_counter = Counter()\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Update the trigram counter\n",
    "        trigram_pad_sent = [\"<s>\"]*2 + sent +  ['<e>']*2\n",
    "        trigram_counter.update([(gram1, gram2, gram3) for gram1, gram2, gram3 in zip(trigram_pad_sent, trigram_pad_sent[1:], trigram_pad_sent[2:])]) \n",
    "    \n",
    "    return trigram_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74c28b",
   "metadata": {},
   "source": [
    "##### *Function to calculate cross-entropy and perplexity*\n",
    "\n",
    "- **Cross-Entropy**\n",
    "\n",
    "$H_{\\alpha}(P, Q) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log_2\\left(\\frac{{\\text{{count}}(x_i, x_{i-1}) + \\alpha}}{{\\text{{count}}(x_{i-1}) + \\alpha \\cdot \\text{{vocab\\_size}}}}\\right)$\n",
    "\n",
    "\n",
    "- **Perplexity**\n",
    "\n",
    "$\\text{PP}(P, Q) = 2^{H(P, Q)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43bdee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crossentropy_perplexity(unigram_counter, bigram_counter, trigram_counter, vocab_size, sentences, alpha, lm):\n",
    "    # Initialize counters and variables based on the chosen language model (unigram/bigram/trigram)\n",
    "    if lm == \"bigram\":\n",
    "        sum_prob = 0\n",
    "        bigram_cnt = 0\n",
    "        for sentence in sentences:\n",
    "            sent = ['<start>'] + sentence + ['<end>']\n",
    "            # Iterate over the bigrams of the sentence\n",
    "            for idx in range(1, len(sent)):\n",
    "                # Calculate the probability of the bigram using add-alpha smoothing\n",
    "                bigram_prob = (bigram_counter[(sent[idx - 1], sent[idx])] + alpha) / (\n",
    "                        unigram_counter[(sent[idx - 1],)] + alpha * vocab_size)\n",
    "                sum_prob += math.log2(bigram_prob)\n",
    "                bigram_cnt += 1\n",
    "        # Calculate cross-entropy and perplexity\n",
    "        cross_entropy = -sum_prob / bigram_cnt\n",
    "        perplexity = math.pow(2, cross_entropy)\n",
    "    else:\n",
    "        sum_prob = 0\n",
    "        trigram_cnt = 0\n",
    "        for sentence in sentences:\n",
    "            sent = ['<start>'] + ['<start>'] + sentence + ['<end>']\n",
    "            # Iterate over the trigrams of the sentence\n",
    "            for idx in range(2, len(sent) - 1):\n",
    "                # Calculate the probability of the trigram using add-alpha smoothing\n",
    "                trigram_prob = (trigram_counter[(sent[idx - 2], sent[idx - 1], sent[idx])] + alpha) / (\n",
    "                        bigram_counter[(sent[idx - 2], sent[idx - 1])] + alpha * vocab_size)\n",
    "                sum_prob += math.log2(trigram_prob)\n",
    "                trigram_cnt += 1\n",
    "        # Calculate cross-entropy and perplexity\n",
    "        cross_entropy = -sum_prob / trigram_cnt\n",
    "        perplexity = math.pow(2, cross_entropy)\n",
    "\n",
    "    return cross_entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7601ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_alpha(unigram_counter, bigram_counter, trigram_counter, vocab_size, sentences, lm):\n",
    "    # Generate a range of alpha values\n",
    "    alphas = np.linspace(0.001, 0.1, 100)\n",
    "    \n",
    "    # Calculate cross-entropy and perplexity for each alpha\n",
    "    values = [calculate_crossentropy_perplexity(unigram_counter, bigram_counter, trigram_counter, vocab_size, sentences, a, lm) for a in alphas]\n",
    "    \n",
    "    # Separate cross-entropy and perplexity values\n",
    "    crossentropy = list(map(itemgetter(0), values))\n",
    "    perplexity = list(map(itemgetter(1), values))\n",
    "    \n",
    "    # Find the minimum cross-entropy and its corresponding alpha\n",
    "    min_crossentropy = min(crossentropy)\n",
    "    index_min_crossentropy = crossentropy.index(min_crossentropy)\n",
    "    min_alpha = alphas[index_min_crossentropy]\n",
    "    \n",
    "    # Find the minimum perplexity\n",
    "    min_perplexity = min(perplexity)\n",
    "    \n",
    "    return min_crossentropy, min_perplexity, min_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdcbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate counters from the training set\n",
    "unigram_counter = calculate_unigram_counter(training)\n",
    "bigram_counter = calculate_bigram_counter(training)\n",
    "trigram_counter = calculate_trigram_counter(training)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127109c",
   "metadata": {},
   "source": [
    "##### *Find Optimal Alpha for the bigram and trigram language models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a67550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================  Bigram  ==================================================\n",
      "Cross Entropy: 7.135\n",
      "Perplexity: 140.579\n",
      "The optimal cross entropy for Bigram, 7.1352 is achieved for alpha equal to: 0.035\n",
      "\n",
      "\n",
      "==================================================  Trigram  ==================================================\n",
      "Cross Entropy: 8.167\n",
      "Perplexity: 287.434\n",
      "The optimal cross entropy for Bigram, 8.1671 is achieved for alpha equal to: 0.012\n"
     ]
    }
   ],
   "source": [
    "# best alpha for bigram\n",
    "crossentropy_bigram, perplexity_bigram, alpha_bigram = tune_alpha(unigram_counter, bigram_counter, trigram_counter, vocab_size, development,\"bigram\")\n",
    "\n",
    "# best alpha for trigram\n",
    "crossentropy_trigram,perplexity_trigram,alpha_trigram = tune_alpha(unigram_counter, bigram_counter, trigram_counter, vocab_size, development,\"trigram\")\n",
    "\n",
    "print('='*50, ' Bigram ', '='*50)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(crossentropy_bigram))\n",
    "print(\"Perplexity: {0:.3f}\".format(perplexity_bigram))\n",
    "print(f\"The optimal cross entropy for Bigram, {round(crossentropy_bigram, 4)} is achieved for alpha equal to: {round(alpha_bigram, 4)}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('='*50, ' Trigram ', '='*50)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(crossentropy_trigram))\n",
    "print(\"Perplexity: {0:.3f}\".format(perplexity_trigram))\n",
    "print(f\"The optimal cross entropy for Bigram, {round(crossentropy_trigram, 4)} is achieved for alpha equal to: {round(alpha_trigram, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdcb314",
   "metadata": {},
   "source": [
    "##### *Calculate Cross-Entropy and Perplexity for the training set (tuned alpha)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b22f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================  Bigram  ==================================================\n",
      "Cross Entropy in Training set: 5.694\n",
      "Perplexity in Training set: 51.776\n",
      "\n",
      "\n",
      "==================================================  Trigram  ==================================================\n",
      "Cross Entropy in Training set: 4.578\n",
      "Perplexity in Training set: 23.881\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "crossentropy_training_bigram,perplexity_training_bigram = calculate_crossentropy_perplexity(unigram_counter, bigram_counter, trigram_counter, vocab_size, training,alpha_bigram,'bigram')\n",
    "\n",
    "#trigram\n",
    "crossentropy_training_trigram,perplexity_training_trigram = calculate_crossentropy_perplexity(unigram_counter, bigram_counter, trigram_counter, vocab_size, training,alpha_trigram,'trigram')\n",
    "\n",
    "print('='*50, ' Bigram ', '='*50)\n",
    "print(\"Cross Entropy in Training set: {0:.3f}\".format(crossentropy_training_bigram))\n",
    "print(\"Perplexity in Training set: {0:.3f}\".format(perplexity_training_bigram))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('='*50, ' Trigram ', '='*50)\n",
    "print(\"Cross Entropy in Training set: {0:.3f}\".format(crossentropy_training_trigram))\n",
    "print(\"Perplexity in Training set: {0:.3f}\".format(perplexity_training_trigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe75c67",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "##### *Calculate Cross-Entropy and Perplexity for the test set (tuned alpha)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26305a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================  Bigram  ==================================================\n",
      "Cross Entropy in Training set: 6.978\n",
      "Perplexity in Training set: 126.096\n",
      "\n",
      "\n",
      "==================================================  Trigram  ==================================================\n",
      "Cross Entropy in Training set: 7.846\n",
      "Perplexity in Training set: 230.126\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "crossentropy_testing_bigram,perplexity_testing_bigram = calculate_crossentropy_perplexity(unigram_counter, bigram_counter, trigram_counter, vocab_size, testing,alpha_bigram,'bigram')\n",
    "\n",
    "#trigram\n",
    "crossentropy_testing_trigram,perplexity_testing_trigram = calculate_crossentropy_perplexity(unigram_counter, bigram_counter, trigram_counter, vocab_size, testing,alpha_trigram,'trigram')\n",
    "\n",
    "print('='*50, ' Bigram ', '='*50)\n",
    "print(\"Cross Entropy in Training set: {0:.3f}\".format(crossentropy_testing_bigram))\n",
    "print(\"Perplexity in Training set: {0:.3f}\".format(perplexity_testing_bigram))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('='*50, ' Trigram ', '='*50)\n",
    "print(\"Cross Entropy in Training set: {0:.3f}\".format(crossentropy_testing_trigram))\n",
    "print(\"Perplexity in Training set: {0:.3f}\".format(perplexity_testing_trigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b391ba",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "##### *Function to calculate the normalized edit distance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81882265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_edit_distance(sentence, vocab):\n",
    "    # Create an array to store probabilities, initialized with infinity\n",
    "    probability_array = np.full((len(sentence.split()), len(vocab)), np.inf)\n",
    "\n",
    "    # Iterate over the words in the sentence\n",
    "    for idx, x in enumerate(probability_array):\n",
    "        # Iterate over the words in the vocabulary\n",
    "        for idy, y in enumerate(x):\n",
    "            # Calculate the probability based on the edit distance between the sentence word and vocabulary word\n",
    "            probability_array[idx][idy] = 1 / (edit_distance(sentence.split()[idx], vocab[idy][0]) + 1)\n",
    "\n",
    "    return probability_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1ae57",
   "metadata": {},
   "source": [
    "##### *Function for the beam search decoder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d235bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(predictions, top_k = 3):\n",
    "    #start with an empty sequence with zero score\n",
    "    output_sequences = [([], 0)]\n",
    "    \n",
    "    #looping through all the predictions\n",
    "    for token_probs in predictions:\n",
    "        new_sequences = []\n",
    "        # print(token_probs)\n",
    "        \n",
    "        #append new tokens to old sequences and re-score\n",
    "        for old_seq, old_score in output_sequences:\n",
    "            # print('old_seq',old_seq)\n",
    "            # print('old_score',old_score)\n",
    "            for char_index in range(len(token_probs)):\n",
    "                # print('char_index',char_index)\n",
    "                new_seq = old_seq + [char_index]\n",
    "                # print('new_seq',new_seq)\n",
    "                #considering log-likelihood for scoring\n",
    "                new_score = old_score + math.log(token_probs[char_index])\n",
    "                # print('token_probs[char_index]',token_probs[char_index])\n",
    "                # print('math_token_probs',math.log(token_probs[char_index]))\n",
    "                # print('new_score',new_score)\n",
    "                new_sequences.append((new_seq, new_score))\n",
    "                # print('new_sequences',new_sequences)\n",
    "                \n",
    "        #sort all new sequences in the de-creasing order of their score\n",
    "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
    "        # print('output_sequences',output_sequences)\n",
    "        \n",
    "        #select top-k based on score \n",
    "        # *Note- best sequence is with the highest score\n",
    "        output_sequences = output_sequences[:top_k]\n",
    "        \n",
    "    return output_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f79ac",
   "metadata": {},
   "source": [
    "##### *Function to get bigram probabilities*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8334e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_probabilities(sentences, alpha):\n",
    "    for sentence in sentences:\n",
    "        sum_prob2 = 0\n",
    "        sent = ['<start>'] + sentence + ['<end>']\n",
    "        # Iterate over the bigrams of the sentence\n",
    "        for idx in range(1, len(sent)):\n",
    "        # calculating the prob while considering the prob of preceding word occuring with the unigram counter\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +alpha) / (unigram_counter[(sent[idx-1],)] + alpha*vocab_size)\n",
    "            sum_prob2 += math.log2(bigram_prob)\n",
    "    return sum_prob2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7573d",
   "metadata": {},
   "source": [
    "##### *Function to find the top-k most probable sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3492c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(test_sentence ,distance_array, top_k, vocabulary):\n",
    "    \n",
    "    # get top-k beam search decoder sequences\n",
    "    beam_decoder = beam_search_decoder(distance_array, top_k = top_k)\n",
    "    \n",
    "    # get words from the vocabulary\n",
    "    beam_decoder_translated = [([vocabulary[k][0] for k in x[0]],x[1]) for x in beam_decoder]\n",
    "    \n",
    "    # construct sentences \n",
    "    beam_decoder_sentences = list(map(itemgetter(0), beam_decoder_translated))\n",
    "    \n",
    "    # get probabilities\n",
    "    beam_decoder_lm_prob = [calculate_bigram_probabilities([sentence],alpha_bigram) for sentence in beam_decoder_sentences]\n",
    "    \n",
    "    # merge sentences and probabilities\n",
    "    final_beam_decoder = [(beam_decoder_translated[i][0], beam_decoder_translated[i][1]+beam_decoder_lm_prob[i]) for i in range(0, len(beam_decoder_lm_prob))]\n",
    "    \n",
    "    # find out-of-vocabulary words\n",
    "    words_oov = []\n",
    "    for idx,i in enumerate(test_sentence.split()):\n",
    "        if (list(map(itemgetter(0),vocabulary)).count(i) == 0):\n",
    "            words_oov.append(test_sentence.split()[idx])\n",
    "    \n",
    "    # sort based on the probability\n",
    "    final_beam_decoder_sorted = sorted(final_beam_decoder, key = lambda val: val[1], reverse = True)\n",
    "    \n",
    "    # print initial sentence\n",
    "    print('Test sentence:              ',test_sentence)\n",
    "    print('Words not in vocabulary:    ', ' '.join(map(str,words_oov)))\n",
    "    print('\\n')\n",
    "    print('='*100)\n",
    "    print('\\n')\n",
    "    \n",
    "    # print each possible sentence\n",
    "    for i in range(top_k):\n",
    "    \n",
    "        print('Possible Sentence Number ', i+1 ,': ', \" \".join(map(str,final_beam_decoder_sorted[i][0])))\n",
    "        print('Score: {:.4f}'.format(final_beam_decoder_sorted[i][1]))\n",
    "        print('\\n')\n",
    "    \n",
    "    return words_oov, final_beam_decoder_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80565eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence:               Today I realy saw a milion people.\n",
      "Words not in vocabulary:     Today realy saw milion people.\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Possible Sentence Number  1 :  today I really law a million people\n",
      "Score: -91.5305\n",
      "\n",
      "\n",
      "Possible Sentence Number  2 :  today I real law a million people\n",
      "Score: -91.6133\n",
      "\n",
      "\n",
      "Possible Sentence Number  3 :  today I really say a million people\n",
      "Score: -91.9945\n",
      "\n",
      "\n",
      "Possible Sentence Number  4 :  today I real say a million people\n",
      "Score: -92.0773\n",
      "\n",
      "\n",
      "Possible Sentence Number  5 :  today I real say a billion people\n",
      "Score: -96.8213\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define test sentence \n",
    "test_sentence = 'Today I realy saw a milion people.'#'Today I saw a milion people realy.'\n",
    "\n",
    "#calculate distance\n",
    "edit_distance_array = calculate_normalized_edit_distance(test_sentence,vocabulary)\n",
    "\n",
    "# spell correction\n",
    "words_oov, alternative_sentences = spell_correction(test_sentence,edit_distance_array,5,vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
