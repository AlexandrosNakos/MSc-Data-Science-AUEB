{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d7d839",
   "metadata": {},
   "source": [
    "# Image Feature Extraction via PCA & kPCA\n",
    "\n",
    "> *Numerical Optimization and Large Scale Linear Algebra*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "For this task, we use images from the Yale Face Database B, which contains $5760$ single light source gray-level images of $10$ subjects, each seen under $576$ viewing conditions. We take $51$ images of the first subject and $51$ images of the third subject as the training data, and $13$ images of each of them as testing data. Then all the images are aligned, and each image has $168 \\times 192$ pixels. We use the $168 \\times 192$ pixel intensities as the original features for each image, thus the original feature vector is $32256$-dimensional. Then we use standard PCA and Gaussian kernel PCA to extract the $9$ most significant features from the training data, and record the eigenvectors. For standard PCA, only the eigenvectors are needed to extract features from testing data. For Gaussian kernel PCA, both the eigenvectors and the training data are needed to extract features from testing data. Note that for standard PCA, there are particular fast algorithms to compute the eigenvectors when the dimensionality is much higher than the number of data points. For kernel PCA, we use a Gaussian kernel with $\\sigma = 22546$. For classification, we use the simplest linear classifier.\n",
    "\n",
    "Parameter selection for kernel PCA directly determines the performance of the algorithm. For Gaussian kernel PCA, the most important parameter is the $\\sigma$ in the kernel function defined by $\\kappa(x,y)=\\exp(-\\|x-y\\|^{2} \\div 2\\sigma^{2})$. The Gaussian kernel is a function of the distance $\\| x-y \\|$ between two vectors $x$ and $y$. Ideally, if we want to separate different classes in the new feature space, then the parameter $\\sigma$ shoud be smaller than inter-class distances, and larger than inner-class distances. However, we don't know how many classes are there in the data, thus it is not easy to estimate the inter-class or inner class distances. Alternatively, we can set $\\sigma$ to a small value to capture only the neighborhood information of each data point. For this purpose, for each data point $x_{i}$, let the distance from $x_{i}$ to its nearest neighbor be $d_{i}^{NN}$. In the experiments, we use this parameter selection strategy:<br><br>$$\\sigma=5mean(d_{i}^{NN}).$$<br>This strategy, in the experiments, ensures that the $\\sigma$ is large enough to capture neighboring data points, and is much smaller than inter-class distances. When using different datasets, this strategy may need modifications.\n",
    "\n",
    "> [***Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models***](https://github.com/sapaladas/msc_data_science/blob/main/q3-numerical_optimization_and_large_scale_linear_algebra/image_feature_extraction_via_pca_kpca/readings/kernel_principal_component_analysis.pdf)  \n",
    "> *Quan Wang, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY 12180 USA*  \n",
    "> *arXiv:1207.3538 \\[cs.CV\\], 2012*\n",
    "\n",
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "959957ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import h5py\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03736d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbfa1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (102, 32256)\n",
      "y_train.shape: (102, 1)\n",
      "x_test.shape:  (26, 32256)\n",
      "y_test.shape:  (26, 1)\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "faces_data = h5py.File('./Data/YaleFaceData.mat', 'r')\n",
    "\n",
    "# train and test set\n",
    "x_train, y_train = faces_data['train_x'][:].T, faces_data['train_t'][:]\n",
    "x_test, y_test = faces_data['test_x'][:].T, faces_data['test_t'][:]\n",
    "\n",
    "# shapes\n",
    "print(f'x_train.shape: {x_train.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'x_test.shape:  {x_test.shape}')\n",
    "print(f'y_test.shape:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4d60a",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "The **principal components** of a set of points in a real coordinate space form a series of $p$ unit vectors. The $i$-th vector represents the direction of a line that optimally fits the data while being orthogonal to the preceding $i-1$ vectors. A best-fitting line, in this context, minimizes the average squared distance from the points to the line. These vectors create an orthonormal basis where individual dimensions of the data are linearly uncorrelated. **Principal Component Analysis (PCA)** involves computing these principal components and utilizing them to transform the data's basis. Often, only the initial principal components are considered, and the rest are disregarded.\n",
    "\n",
    "##### *Function for PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "818d36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x_train, x_test, components_num = 9):\n",
    "    \n",
    "    # initialize PCA\n",
    "    pca = PCA(n_components=components_num, random_state=123)\n",
    "    \n",
    "    # fit PCA\n",
    "    pca.fit(x_train)\n",
    "    \n",
    "    # get principal components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # data transformation\n",
    "    x_train_PCA = np.dot(x_train, eigenvectors.T)\n",
    "    x_test_PCA = np.dot(x_test, eigenvectors.T)\n",
    "    \n",
    "    return x_train_PCA, x_test_PCA\n",
    "\n",
    "# execute function\n",
    "x_train_pca, x_test_pca = pca(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577070ee",
   "metadata": {},
   "source": [
    "## Gaussian Kernel PCA \n",
    "\n",
    "**Kernel Principal Components Analysis (kernel PCA)** is an advancement in multivariate statistics, extending Principal Components Analysis (PCA) to handle non-linear data separability through the application of kernels. The fundamental concept involves projecting non-linearly separable data onto a higher-dimensional space, transforming it into a linearly separable form.\n",
    "\n",
    "##### *Implementation*\n",
    "\n",
    "1. Use Gaussian kernel PCA to project the train data onto a lower dimensional space and record the eigenvectors\n",
    "2. Use Gaussian kernel PCA and the eigenvectors obtained from previous step, to project the test data onto a lower dimensional space\n",
    "\n",
    "##### *Function for Kernel PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff3fb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the Radial Basis Function Kernel\n",
    "def gaussian_rbf_kernel(x, sigma = 22546):\n",
    "\n",
    "    D = pdist(x,'sqeuclidean')\n",
    "    D = squareform(D)\n",
    "    D = np.where(D<0,0,D)\n",
    "    K = np.sqrt(D)\n",
    "    K = K**2\n",
    "    K = np.exp(-K/(2*sigma**2))\n",
    "    \n",
    "    return K\n",
    "\n",
    "def kernel_PCA(x, sigma = 22546, components_num = 9):\n",
    "    \n",
    "    # construct the kernel matrix\n",
    "    K0 = gaussian_rbf_kernel(x)\n",
    "    \n",
    "    # compute the gram matrix\n",
    "    N = x.shape[0]\n",
    "    oneN = np.ones((N,N))/N\n",
    "    K = K0 - np.dot(oneN,K0) - np.dot(K0,oneN) + np.dot(np.dot(oneN,K0),oneN)\n",
    "    \n",
    "    # eigenvalue analysis\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(K)\n",
    "    \n",
    "    # normalization\n",
    "    norm_eigenvectors = np.sqrt(sum(eigenvectors**2))\n",
    "    eigenvectors = eigenvectors / np.tile(norm_eigenvectors, (eigenvectors.shape[0], 1))\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    kLargestIdx = np.argsort(eigenvalues)[::-1][:components_num]\n",
    "    eigenvectors = eigenvectors[:,kLargestIdx]\n",
    "    x_train_kPCA = np.dot(K0,eigenvectors)\n",
    "    \n",
    "    return x_train_kPCA, eigenvectors\n",
    "\n",
    "# execute function\n",
    "x_train_kPCA, eigenvectors = kernel_PCA(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cd347",
   "metadata": {},
   "source": [
    "##### *Project the test data to a lower dimensional space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8ffa931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_projection(x_train, x_test, sigma = 22546):\n",
    "\n",
    "    D = pdist(np.concatenate([x_train,x_test]), 'sqeuclidean')\n",
    "    D = squareform(D)\n",
    "    D = np.where(D<0,0,D)\n",
    "    K = np.sqrt(D)\n",
    "    N = x_train.shape[0]\n",
    "    K = K[N:,:N]\n",
    "    K = K**2\n",
    "    K = np.exp(-K/(2*sigma**2))\n",
    "    \n",
    "    return K\n",
    "\n",
    "def kPCA_projection(x_train, x_test, eigenvectors, sigma = 22546):\n",
    "\n",
    "    # construct the kernel matrix\n",
    "    K = rbf_kernel_projection(x_train, x_test, sigma)\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    x_test_kPCA = np.dot(K,eigenvectors)\n",
    "    \n",
    "    return x_test_kPCA\n",
    "\n",
    "# execute function\n",
    "x_test_kPCA = kPCA_projection(x_train, x_test, eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fefc73",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09c6bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA train error: 8.82%\n",
      "PCA  test error: 23.08%\n",
      "\n",
      "Kernel PCA train error: 6.86%\n",
      "Kernel PCA  test error: 11.54%\n"
     ]
    }
   ],
   "source": [
    "def get_error_rates(model, x, y, x_pred, y_true):\n",
    "    # initialize model\n",
    "    model = LinearRegression()\n",
    "    # fit the model in the data\n",
    "    model.fit(x, y.ravel())\n",
    "    # make predictions\n",
    "    predictions = model.predict(x_pred)\n",
    "    # translate prediction\n",
    "    predictions = np.where(predictions > 0, 1, -1)\n",
    "    # compute error rate\n",
    "    error_rate = 1 - accuracy_score(y_true, predictions)\n",
    "    return error_rate\n",
    "\n",
    "# PCA train\n",
    "error_rate = get_error_rates(LinearRegression(), x_train_pca, y_train, x_train_pca, y_train)\n",
    "print(f'PCA train error: {round(error_rate*100,2)}%')\n",
    "\n",
    "# PCA test\n",
    "error_rate = get_error_rates(LinearRegression(), x_train_pca, y_train, x_test_pca, y_test)\n",
    "print(f'PCA  test error: {round(error_rate*100,2)}%\\n')\n",
    "\n",
    "# Kernel PCA train\n",
    "error_rate = get_error_rates(LinearRegression(), x_train_kPCA, y_train, x_train_kPCA, y_train)\n",
    "print(f'Kernel PCA train error: {round(error_rate*100,2)}%')\n",
    "\n",
    "# Kernel PCA test\n",
    "error_rate = get_error_rates(LinearRegression(), x_train_kPCA, y_train, x_test_kPCA, y_test)\n",
    "print(f'Kernel PCA  test error: {round(error_rate*100,2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
